{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 598 - Lecture 5: Generative Models for Linear Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time we saw  *Logistic Regression* which was a discriminative learning technique that tried to estimate $P(Y~|~X)$. This time we will learn about **generative learning** where we aim to model $P(X~|~Y)$ and $P(y)$ and use the two together to estimate $P(Y~|~X)$ via Baye's Rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)\n",
    "\n",
    "Assume we have two classes $Y\\in(0,1)$. We can find the probabilities of finding each class:\n",
    "$$P(Y=0) = \\frac{N(Y=0)}{N(Y=1) + N(Y=0)}\\\\\n",
    "P(Y=1) = \\frac{N(Y=1)}{N(Y=1) + N(Y=0)}$$\n",
    "\n",
    "In LDA we assume that our data, $X$, was generated by a Multivariate Gaussian:\n",
    "\n",
    "$$P(X~|~Y) = \\frac{ \\exp{ \\Big( -\\frac{1}{2} (x - \\mu )^T \\Sigma^{-1} (x-\\mu) \\Big) }} { (2\\pi)^{\\frac{1}{2}} |\\Sigma|^{\\frac{1}{2}} }$$\n",
    "\n",
    "Note here that the key assumption about LDA is that they have the same covariance matrix $\\Sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (to be written using Mitchells notes)\n",
    "This is another generative learning model. Here our goal is to estimate $P(Y|X)$, i.e. how likely are we to observe a certain class, $Y$, given we certain features $X$.\n",
    "\n",
    "Recall Bayes Rule: $ P(Y~|~X) = \\frac{P(X~|~Y) P(Y)}{P(X)}$. \n",
    "\n",
    "We have three components, \n",
    "$$P(Y) := \\text{The probability of observing class $Y$}\\\\\n",
    "P(X) := \\text{Probability of observing the features $X$}\\\\\n",
    "P(X~|~Y) := \\text{The probability of observing features $X$ in a certain class $Y$}\n",
    "$$\n",
    "\n",
    "**The Naive Bayes Assumption:** Here we assume that the features of $X$, $X_j$ are conditionally independent given $Y$. This means:\n",
    "\n",
    "$$P(X~|~Y) = P(X_1, X_2, X_3, \\ldots, X_m | Y) = \\prod_{j=1}^n P(X_j~|~Y) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus now: \n",
    "\n",
    "$$P(Y~|~X) = \\frac{P(X~|~Y) P(Y)}{P(X)} = \\frac{P(Y) \\prod_{j=1}^n P(X_j~|~Y) }{P(X)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our goal now is to estimate the individual $P(X_j~|~Y)$ and $P(Y)$ from the data. We define the estimators:\n",
    "$$\n",
    "\\theta_1 := P(Y=1) \\\\\n",
    "\\theta_{j,1} := P(X_j=1~|~Y=1) \\\\\n",
    "\\theta_{j,0} := P(X_j=0~|~Y=0)\n",
    "$$\n",
    "\n",
    "Above which can be obtained by counting. We now need an evaluation criteria. For example, here we can find the parameters that maximize the log-likelihood function:\n",
    "\n",
    "Likelihood: $P(Y~|~X) \\propto \\prod_{i=1}^n \\Big( P(y_i) \\prod_{j=1}^m P(x_{i,j}~|~y_i) \\Big)$\n",
    "Reasoning:\n",
    "- Bayes theorem, $P(X)$ in the denominator is disregarded.\n",
    "- The samples $i$ are independent, thus we take a product over the $n$ $y_i$'s\n",
    "- Input features are independent, thus we can take a product over $m$ for each feature $x_{i,j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-likelihood: $\\log L(\\theta_1, \\theta_{j,1}, \\theta_{j,0}) = \\sum_{i=1}^n \\Big( \\log P(y_i) + \\sum_{j=1}^m \\log P(x_{i,j}~|~y_i) \\Big)$\n",
    "\n",
    "- To estimate $\\theta_1$, we need to take the derivative: of $\\log L$: \n",
    "$$\\frac{\\delta L}{\\delta \\theta_1} = \\sum_{j=1}^n \\Big(\\frac{y_i}{\\theta_1} - \\frac{(1-y_i)}{(1-\\theta_1)} \\Big) = 0 $$\n",
    "\n",
    "Solving for $\\theta_1$, we obtain: $\\theta_1 = \\frac{1}{n} \\sum_{i=1}^n y_i$ i.e the proportion of examples where $y=1$. By using simillar logic we can find $\\theta_{j,1}$ and $\\theta_{j,0}$ which are the proportion of examples where $x_j = 1$ and $y = \\{0,1\\}$ respectively.\n",
    "\n",
    "Now how do we make decisions? Consider the log-odds ratio:\n",
    "\n",
    "$$ \n",
    "\\log \\frac{P(X~|~Y=1) P(Y=1)}{P(X~|~Y=0) P(Y=0)} = \\log \\frac{ P(Y=1) }{ P(Y=0) } + \\log \\frac{ \\prod_{j=1}^m P(x_j~|~Y=1) }{ \\prod_{j=1}^m P(x_j~|~y=0) }\\\\\n",
    "= log \\frac{ P(Y=1) }{ P(Y=0) } + \\sum_{j=1}^m \\log \\frac{  P(x_j~|~Y=1) }{ P(x_j~|~y=0) }\n",
    "$$\n",
    "\n",
    "Now notice that: **NEED TO DO WORK HERE ON DERIVATION**\n",
    "$$\\sum_{j=1}^m \\log \\frac{  P(x_j~|~Y=1) }{ P(x_j~|~y=0)} = $$\n",
    "\n",
    "By letting $w_{j,0} = \\log \\frac{ P( x_j = 0 | y = 1) } {P(x_j = 0 | y = 0)}$ and $w_{j,1} = \\log \\frac{ P(x_j=1 | y=1) }{ P(x_j=1 | y=0) }$ we obtain:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = log \\frac{ P(Y=1) }{ P(Y=0) } + \\sum_{j=1}^m \\Big( w_{j,0}(1-x_j) + w_{j,1}x_j  \\Big)\\\\\n",
    "= \\underbrace{log \\frac{ P(Y=1) }{ P(Y=0) } + \\sum_{j=1}^m w_{j,0} }_{w_0} + \\underbrace{ \\sum_{j=1}^m \\Big( w_{j,1} - w_{j,0}\\Big)x_j }_{X^TW} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_0$ and $W$ are the weights of a linear boundary give our features $x_1, \\ldots, x_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example: Learning a Boolean Function:\n",
    "\n",
    "Assume we have $y^* = x_1\\oplus x_2$, the logical operator for XOR, exclusive OR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([np.random.randint(2, size=50), np.random.randint(2, size=50)])\n",
    "Y = 1*np.logical_xor( X[0], X[1] )\n",
    "X = X.T\n",
    "\n",
    "\n",
    "# unique labels in our set\n",
    "# this will hold the references for the label indicies.\n",
    "unique_labels = np.unique(Y) \n",
    "\n",
    "data_array = np.array(X) # the data in array format\n",
    "\n",
    "\n",
    "n = data_array.shape[0] # number of observations we have\n",
    "m = data_array.shape[1] # number of features\n",
    "k = len(unique_labels) # number of classes\n",
    "t = 2 #assuming x_i are boolean variables\n",
    "\n",
    "labels_array = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 1 #smoothening parameter\n",
    "\n",
    "theta = np.zeros((m,t,k))\n",
    "pie = np.zeros(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_k= 0\n",
      "pie[k]= 0.384615384615\n",
      "theta(j= 0 , t= 0 k= 0 ) =  0.666666666667\n",
      "theta(j= 0 , t= 1 k= 0 ) =  0.333333333333\n",
      "theta(j= 1 , t= 0 k= 0 ) =  0.666666666667\n",
      "theta(j= 1 , t= 1 k= 0 ) =  0.333333333333\n",
      "y_k= 1\n",
      "pie[k]= 0.615384615385\n",
      "theta(j= 0 , t= 0 k= 1 ) =  0.545454545455\n",
      "theta(j= 0 , t= 1 k= 1 ) =  0.454545454545\n",
      "theta(j= 1 , t= 0 k= 1 ) =  0.454545454545\n",
      "theta(j= 1 , t= 1 k= 1 ) =  0.545454545455\n"
     ]
    }
   ],
   "source": [
    "unique_feature_labels = [None]*m # array to hold the unique values of each feature\n",
    "\n",
    "for label_index, label in enumerate(unique_labels):\n",
    "    print 'y_k=',label\n",
    "    matching_indicies = labels_array == label\n",
    "    pie[label_index] = (np.sum(matching_indicies)+gamma)/(float(n)+gamma*k)\n",
    "    print 'pie[k]=',pie[label_index]\n",
    "    # obtain all the data that matches our labels.\n",
    "    data_for_label = data_array[matching_indicies]\n",
    "    \n",
    "    # transpose the matrix because we want to now calculate the counts of the features.\n",
    "    feature_observations = data_for_label.T\n",
    "    \n",
    "    for feature_index, feature_observation in enumerate(feature_observations):\n",
    "        \n",
    "        unique_feature_labels[feature_index] = list(sorted(np.unique(feature_observation)))\n",
    "        \n",
    "        for count in Counter(feature_observation).items():\n",
    "            t_instance, N_condition = count[0], count[1] # value, frequency \n",
    "\n",
    "            theta[feature_index, t_instance, label_index] = (N_condition + gamma)/float(np.sum(matching_indicies) + gamma * t)\n",
    "            print 'theta(j=',feature_index,', t=',t_instance,'k=',label_index,') = ',theta[feature_index, t_instance, label_index]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.66666667,  0.54545455],\n",
       "        [ 0.33333333,  0.45454545]],\n",
       "\n",
       "       [[ 0.66666667,  0.45454545],\n",
       "        [ 0.33333333,  0.54545455]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_classify: [0 0]\n",
      "P(y= 0 )= 0.384615384615\n",
      "P(X_ 0 = 0  | y= 0 )= 0.666666666667\n",
      "P(X_ 1 = 0  | y= 0 )= 0.666666666667\n",
      "\n",
      "\n",
      "P(y= 1 )= 0.615384615385\n",
      "P(X_ 0 = 0  | y= 1 )= 0.545454545455\n",
      "P(X_ 1 = 0  | y= 1 )= 0.454545454545\n",
      "\n",
      "\n",
      "Ps:  [(0, 0.17094017094017092), (1, 0.15257469802924348)]\n",
      "Prediction: (0, 0.17094017094017092)\n",
      "Actual: 0\n"
     ]
    }
   ],
   "source": [
    "idx = 7\n",
    "classify_this = X[idx]\n",
    "print 'to_classify:',classify_this\n",
    "results = []\n",
    "for label in unique_labels:\n",
    "    p = pie[label]\n",
    "    print 'P(y=',label,')=',pie[label]\n",
    "    for j, val in zip(range(m), classify_this):\n",
    "        print 'P(X_',j,'=',val,' | y=',label,')=', theta[j,val,label]\n",
    "        p = p * theta[j,val,label]\n",
    "    print '\\n'\n",
    "    results.append((label, p))\n",
    "\n",
    "print 'Ps: ',results\n",
    "\n",
    "print 'Prediction:', max(results,key=lambda l: l[1])\n",
    "print 'Actual:',Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31402814,  0.68597186]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb = BernoulliNB()\n",
    "nb_trained = nb.fit(X, Y.T)\n",
    "nb_trained.predict_proba(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, array([0, 1]), array([1]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0], X[0], nb_trained.predict(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, array([0, 0]), array([0]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[1], X[1], nb_trained.predict(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, array([1, 0]), array([1]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[2], X[2], nb_trained.predict(X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, array([1, 1]), array([1]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[8], X[8], nb_trained.predict(X[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good check is if our probability estimates are good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.38,  0.62])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(nb_trained.class_log_prior_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.38461538,  0.61538462])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks GOOD! How about our feature estimates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33333333,  0.33333333],\n",
       "       [ 0.45454545,  0.54545455]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(nb_trained.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.66666667,  0.54545455],\n",
       "        [ 0.33333333,  0.45454545]],\n",
       "\n",
       "       [[ 0.66666667,  0.45454545],\n",
       "        [ 0.33333333,  0.54545455]]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty decent if you match up which ones are the positive labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example: Sex Determination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([['Drew', 'No', 'Blue', 'Short', 'Male',],\n",
    "['Claudia', 'Yes', 'Brown', 'Long', 'Female',],\n",
    "['Drew', 'No', 'Blue', 'Long', 'Female',],\n",
    "['Drew', 'No', 'Blue', 'Long', 'Female',],\n",
    "['Alberto', 'Yes', 'Brown', 'Short', 'Male',],\n",
    "['Karin', 'No', 'Blue', 'Long', 'Female',],\n",
    "['Nina', 'Yes', 'Brown', 'Short', 'Female',],\n",
    "['Sergio', 'Yes', 'Blue', 'Long', 'Male',]])\n",
    "df.columns = ['Name', 'isOver170', 'EyeColor', 'HairLength', 'Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>isOver170</th>\n",
       "      <th>EyeColor</th>\n",
       "      <th>HairLength</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drew</td>\n",
       "      <td>No</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Short</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Claudia</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Long</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drew</td>\n",
       "      <td>No</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Long</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Drew</td>\n",
       "      <td>No</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Long</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alberto</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Short</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Karin</td>\n",
       "      <td>No</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Long</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nina</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Short</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sergio</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Long</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name isOver170 EyeColor HairLength     Sex\n",
       "0     Drew        No     Blue      Short    Male\n",
       "1  Claudia       Yes    Brown       Long  Female\n",
       "2     Drew        No     Blue       Long  Female\n",
       "3     Drew        No     Blue       Long  Female\n",
       "4  Alberto       Yes    Brown      Short    Male\n",
       "5    Karin        No     Blue       Long  Female\n",
       "6     Nina       Yes    Brown      Short  Female\n",
       "7   Sergio       Yes     Blue       Long    Male"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
