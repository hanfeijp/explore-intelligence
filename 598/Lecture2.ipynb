{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP-598 Lecture 2: Linear Regression\n",
    "\n",
    "In general for superviesed learning we have a dataset of training examples $x_i$: $x_i = (x_{i1}, x_{i2}, \\ldots, x_{in}, y_i)$. Here we have $m$ features in each of our training examples. Given such a dataset of $X\\times Y$ we want $f:X\\longrightarrow Y$, where $f$ is the hypothesis function.\n",
    "\n",
    "## Variable Types\n",
    "\n",
    "1. Ordinal - no metric relation between values, allows us ranking / ordering\n",
    "2. Qualitative - Categorical labels for a value.\n",
    "3. Quantitative - A continous real value.\n",
    "\n",
    "## The i.i.d. assumption\n",
    "1. all rows of dataset are independent (or freshly sampled)\n",
    "2. all rows of dataset are identical (they are from the same underlying distribution)\n",
    "\n",
    "- these assumptions are made in supervised learning and sometimes in unsupervised learning but not inreinforcement learning.\n",
    "- this is important because it allows us to assume that errors are independent. \n",
    "\n",
    "## Risk Minimization\n",
    "Given some measure of error or error function $L_s(f)$ that quantifies the errors made by $f$ on a set $S$, we want:\n",
    "$$ERM_F (S) = \\arg\\min_{f\\in F} L_s(f) $$\n",
    "\n",
    "Where $F$ is a class of functions known as the *Hypothesis Class* (eg: lines, parabolas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Linear Functions\n",
    "\n",
    "### Representation\n",
    "\n",
    "Given some weights $w$ and feature set $X$, we propose the following hypothesis function: $f_w (X) = w_0 + w_1 x_1 + \\ldots + w_m x_m$. How can we handle $w_0$? We simply add an extra feature $x_0 := 1$ which is known as the bias term. We can now simplify:\n",
    "\n",
    "$$f_w (X) = \\Sigma_{j=1}^{m} w_jx_j$$\n",
    "\n",
    "We can write this in matrix form: $f_w(X) = Xw$, where $X$ has dimensions $n\\times m$ and $w$ and dimensions $m \\times 1$\n",
    "\n",
    "### Evaluating the Error in $w$\n",
    "\n",
    "$$ Err(w) = \\Sigma_{i=1}^n (y_i - w^Tx_i)^2 $$\n",
    "\n",
    "Rewritten in matrix notation:\n",
    "\n",
    "$$Err(w) = (Y-Xw)^T(Y-Xw)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(X,Y, W):\n",
    "    return ((Y-X*W).T * (Y-X*W))[0]\n",
    "\n",
    "#x_validate = np.matrix( [ np.ones(50) ,np.linspace(0,1,50) ] ).T\n",
    "#y_validate = np.matrix( np.linspace(0,1,50) + 1 ).T\n",
    "#mean_squared_error(x_validate, y_validate, np.matrix([[1],[1]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing the Error (Optimization)\n",
    "\n",
    "We take the derivative wrto $w$ and equate to $0$:\n",
    "$$\\frac{\\delta Err(w)}{\\delta w} = -2 X^T (Y-Xw) = 0$$\n",
    "$$\\implies X^TY = X^TXw \\implies \\bar{w} = (X^TX)^{-1}(X^TY)$$\n",
    "The weights obtained are estimates and they show how important certain features are relative to each other. the computational cost of this algorithm is dominated by the inversion process and it takes $O(m^3)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_mean_squared_error(X, Y):\n",
    "    \"\"\"calculates the least mean squared error based on \n",
    "        @params:\n",
    "            X, Y = obersvations and outputs.\n",
    "        @returns:\n",
    "            w = the set of weights for a linear function\n",
    "    \"\"\"\n",
    "    return np.linalg.inv(X.T*X)*(X.T*Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array( [ 0.86, 0.09, -0.85 , 0.87, -0.44, -0.43, -1.10 , 0.40, -0.96 , 0.17 ] )\n",
    "bias_term = np.ones(x.shape)\n",
    "y= np.matrix( [ 2.49 , 0.83, -0.25 , 3.10 , 0.87 , 0.02, -0.12 , 1.81, -0.83,  0.43 ] ).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first do it term by term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.  ,  0.86],\n",
       "        [ 1.  ,  0.09],\n",
       "        [ 1.  , -0.85],\n",
       "        [ 1.  ,  0.87],\n",
       "        [ 1.  , -0.44],\n",
       "        [ 1.  , -0.43],\n",
       "        [ 1.  , -1.1 ],\n",
       "        [ 1.  ,  0.4 ],\n",
       "        [ 1.  , -0.96],\n",
       "        [ 1.  ,  0.17]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.matrix( [ bias_term, x ] ).T\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 10.    ,  -1.39  ],\n",
       "        [ -1.39  ,   4.9261]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 8.35  ],\n",
       "        [ 6.4601]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.05881341],\n",
       "        [ 1.61016842]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(X.T * X)* (X.T * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.05881341],\n",
       "        [ 1.61016842]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_mean_squared_error(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_hypothesis(weights):\n",
    "    \"\"\"\n",
    "        returns a function that can be evaluated using X to obtain y\n",
    "    \"\"\"\n",
    "    def h(X):\n",
    "        return X * weights\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_estimate = linear_hypothesis(least_mean_squared_error(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_estimates = f_estimate( np.matrix( [ np.ones(100), np.linspace(-1,1, 100) ] ).T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1065ab390>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1068692d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y, 'x')\n",
    "plt.plot(np.linspace(-1,1,100), y_estimates, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 2.24015903]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How did we do? we can figure out using the mean_squared_error\n",
    "mean_squared_error(X,y, least_mean_squared_error(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "Note that the gradient of the error is the vector indicating the direction to the minimum. We can make steps toward that minimum.\n",
    "See following pseudocode:\n",
    "\n",
    "1. Randomly initialize a weight vector $w_0$\n",
    "2. For (k=1:N)\n",
    "    $$w_{k+1} = w_k - \\alpha_k \\frac{\\delta Err(w_k)}{\\delta w_k}$$\n",
    "3. We end when $|w_{k+1} - w_{k}| < \\epsilon$\n",
    "\n",
    "Convergence depends on the learning rate, $\\alpha_k>0$ for iteration $k$. If it is too large, the algorithim will oscillate forever, if it's too small, it will take too long to converge.\n",
    "\n",
    "We can pick $\\alpha$ based on some theoretical criterion picked dynamically by using the **Robbins-Monroe Conditions** but in most cases it is kept constant.\n",
    "\n",
    "\n",
    "Below we attempt to implement this algorithm using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def derivative_of_squared_error(X,y,w):\n",
    "    return 2 * (X.T * X * w - X.T * y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, min_error=False, alpha=0.5, max_iterations = 200 ,derivative_error_function=derivative_of_squared_error):\n",
    "    number_of_weights = X.shape[1]\n",
    "    w = np.matrix( np.random.random(size=number_of_weights) ).T\n",
    "    for k in range(max_iterations):\n",
    "        w_new = w - alpha * derivative_error_function(X,y,w)\n",
    "        if min_error:\n",
    "            if np.abs(w_new - w) < min_error:\n",
    "                break\n",
    "        w = w_new\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ -3.99452116e+297],\n",
       "        [  1.02258814e+297]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(X,y,max_iterations=10000,alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
