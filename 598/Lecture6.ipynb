{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 598 - Lecture 6: Evaluation Performance\n",
    "\n",
    "## Why do you need a way to evaluate performace?\n",
    "1. Test the performance of algorithms\n",
    "2. Selecting the right model (i.e. Hyperparameters etc)\n",
    "3. Evaluating impact on a new application\n",
    "\n",
    "We have already spoken abou a test set to measure the performance (unbiasedly). Read recommended reading for today.\n",
    "\n",
    "We could also use it when we want to test the hypothesis: \"variable X influences behaviour of variable Y\". \n",
    "\n",
    "**Examples of X:**\n",
    "1. Type of learning algorithm\n",
    "2. Number of degrees of freedom in the polynomial\n",
    "\n",
    "**Examples of Y:**\n",
    "1. Classification accuracy\n",
    "2. Mean squared error\n",
    "\n",
    "\n",
    "How can we tackle it with such an experiment? Vary X and measure Y... is that sufficient? Probably not, think about overfitting: here, increasing the order does lower the error but it's not the best solution!\n",
    "\n",
    "Recall: Test-Train curves.\n",
    "\n",
    "Consider a generic loss function. The *train error* is the average loss on the dataset used to train the algorithm and usually improves with model complexity. The *test error* is the average loss measured on a hold-out dataset and usually improves with more training data.\n",
    "\n",
    "Objective: Minimize the *test* error.\n",
    "\n",
    "## Understanding the Error\n",
    "\n",
    "Consider $Y = h_w(X) + \\epsilon$ where $\\epsilon \\sim N(0,\\sigma^2_\\epsilon)$. Then the error $Err(x) = E[ (Y-h_w(X))^2~|~X = x] = \\sigma^2_\\epsilon + \\text{Bias}^2 + \\text{Variance}$\n",
    "\n",
    "The first term contains the irreducible error (the variance of the target around its true mean). The other two terms can be traded off: Where a simple model has low variance and high bias and a complex model has a low bias and high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the test error: K-fold cross validation\n",
    "\n",
    "1. Split your training set into $k$ subsets of size $n/k$ each.\n",
    "2. Train with $k-1$ subsets and estimate the metric with the remaining subset. \n",
    "3. Repeat $k$ times each time leaving a subset out. \n",
    "4. Average the metrics to obtain the estimate of the metric.\n",
    "\n",
    "When $k=1$ we get a high variance estimate of $Err()$ and is fast to compute. However, when $k>1$, we improve the estimate of $Err()$ but we waste $\\frac{1}{k}$ of the data. It also takes a longer time to compute. When $k=n$, it is called *leave one out*. Here we don't waste data but we have to train $n$ times and is much much slower.\n",
    "\n",
    "### What is the best value for $k$? Consider:\n",
    "1. How many examples you have?\n",
    "2. How long do you want to wait until you get an answer.\n",
    "3. Randomize your data when you do your $k$-fold test-train split because there might be a structure in your dataset. \n",
    "\n",
    "**Do not fit $k$ according to your data**\n",
    "\n",
    "Overfitting thanks to $k$. Given a dataset with 50 examples and 1000 features, we train 1000 linear regression models using a single feature each. The best of those 1000 will look very good. But it would've been good even if the output were random.\n",
    "\n",
    "**How can we avoid this?**\n",
    "\n",
    "When we have many parameters to optimize, we use three datasets: train, validation, test.\n",
    "\n",
    "1. The *training set* is used to train the model.\n",
    "2. The *validation set* is used to estimate the prediction error for the given model and pick the parameters.\n",
    "3. The *test set* is used to estimate the generalization error once the model is fit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the test error: Bootstrapping\n",
    "The general idea of bootstrapping is as follows: Given a dataset $D$ with $N$ examples:\n",
    "1. Randomly draw (with replacement) $B$ datasets of size $N$ from $D$.\n",
    "2. Estimate the metric of interest on each of the $B$ datasets.\n",
    "3. Take the mean of the estimates.\n",
    "\n",
    "In the context of machine learning, we use a dataset $b$ to fit a hypothesis function, $h^b$. We then use the original dataset $D$ to evaluate the error. We then average of all bootstrap sets.\n",
    "\n",
    "$$ \\widehat{Err_{\\text{boot}}} = \\frac{1}{B} \\frac{1}{N} \\sum_{b=1}^B \\sum_{i=1}^N L\\big(y_i, h^b(x_i)\\big)$$\n",
    "\n",
    "Now note that the bootstrap estimate of the error is not good. There is an overlap between the training set $b$ and the test set $D$. This overlap can make overfit predictions look unrealistically good. Now consider the leave-one-out bootstrap method:\n",
    "\n",
    "$$ \\widehat{Err_{\\text{boot-LOO}}} = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{|C^{-i}|} \\sum_{b \\in C^{-i}} L\\big(y_i, h^b(x_i)\\big)$$\n",
    "\n",
    "Where $C_i$ contains all the bootstrap subsets $b$ that do not contain the $i$th observation (to avoid the overlap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Todo:\n",
    "\n",
    "- Classification evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
