{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the iterative version of policy evaluation given by:\n",
    "$$V_{k+1}(s) \\longleftarrow R(s,\\pi(s)) + \\gamma \\sum_{s'\\in S} T(s,\\pi(s), s')V_k(s')$$\n",
    "And we converge when the residual is $\\lt 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function found is: [0.0, 0.0, 24.691358024691358, 199.39214863918315, 187.04646962683745, 163.11719055352549]\n"
     ]
    }
   ],
   "source": [
    "# initial V\n",
    "V = [[0]*6]\n",
    "R = [0,0,20,10,0,-10] # the reward from state S using action A\n",
    "# prob of action success is 0.8, to fail and stay in same state is 0.2\n",
    "T = [\n",
    "    [1,0,0,0,0,0],# prob of going from s1 --> s1 using action L is 1\n",
    "    [0.8,0.2,0,0,0,0], # \n",
    "    [0,0.8,0.2,0,0,0],\n",
    "    [0,0,0,1,0,0], # prob of going from s4 --> s4 using action L is 1\n",
    "    [0,0,0,0.8,0.2,0],\n",
    "    [0,0,0,0,0.8,0.2]\n",
    "]\n",
    "gamma = 0.95\n",
    "for i in range(200):\n",
    "    V.append([0]*6) # add a new row\n",
    "#     print 'iteration:',i,',V=',V[-2]\n",
    "    for s in range(6):\n",
    "        V[-1][s] = R[s] + gamma*sum(np.multiply(T[s], V[-2]))\n",
    "    residual = sum(np.subtract(V[-1], V[-2]))\n",
    "#     print 'residual:',residual\n",
    "    if residual < 0.1:\n",
    "        break\n",
    "print 'Value function found is:',V[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the new policy we need to find:\n",
    "$$\\pi'(s) = \\text{argmax}_{L,R,U,D} (R(s,a) + \\gamma \\sum_{s'\\in S}T(s,a,s') V^\\pi (s')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy is: ['DOWN', 'DOWN', 'DOWN', 'DOWN', 'LEFT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "# first the transition matrix with actions:\n",
    "T = {\n",
    "    'UP':[\n",
    "        [1,0,0,0,0,0], \n",
    "        [0,1,0,0,0,0], \n",
    "        [0,0,1,0,0,0], \n",
    "        [0.8,0,0,0.2,0,0], \n",
    "        [0,0.8,0,0,0.2,0],\n",
    "        [0,0,0.8,0,0,0.2]\n",
    "        ],\n",
    "    'DOWN':[\n",
    "        [0.2,0,0,0.8,0,0],\n",
    "        [0,0.2,0,0,0.8,0],\n",
    "        [0,0,0.2,0,0,0.8],\n",
    "        [0,0,0,1,0,0],\n",
    "        [0,0,0,0,1,0],\n",
    "        [0,0,0,0,0,1]\n",
    "    ],\n",
    "    'LEFT':[\n",
    "        [1,0,0,0,0,0],\n",
    "        [0.8,0.2,0,0,0,0],\n",
    "        [0,0.8,0.2,0,0,0],\n",
    "        [0,0,0,1,0,0],\n",
    "        [0,0,0,0.8,0.2,0],\n",
    "        [0,0,0,0,0.8,0.2]\n",
    "    ],\n",
    "    'RIGHT':[\n",
    "        [0.2,0.8,0,0,0,0],\n",
    "        [0,0.2,0.8,0,0,0],\n",
    "        [0,0,1,0,0,0],\n",
    "        [0,0,0,0.2,0.8,0],\n",
    "        [0,0,0,0,0.2,0.8],\n",
    "        [0,0,0,0,0,1]\n",
    "    ]\n",
    "}\n",
    "# the reward function is the same as above:\n",
    "pi = []\n",
    "# action_set = ['UP','DOWN','LEFT','RIGHT']\n",
    "action_set = ['DOWN', 'LEFT', 'RIGHT', 'UP']\n",
    "for s in range(6): #iterate over all states\n",
    "    action_values = []\n",
    "    for action in action_set:\n",
    "        action_values.append(R[s] + gamma * sum(np.multiply(T[action][s], V[-1])))\n",
    "    pi.append(action_set[np.argmax(action_values)])\n",
    "\n",
    "print 'Policy is:',pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than by vision, to find the optimal policy we can use value iteration where we find the optimal policy at each step:\n",
    "$$V_k(s) = \\text{max}_{L,R,U,D} R(s,a) + \\gamma \\sum_{s' \\in S} \\big( T(s,a,s')V_{k-1}(s')\\big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value function is: [349.77323016524207, 372.94043028717471, 397.63178831186599, 340.38178313725592, 349.77323016524207, 360.59475127482904]\n",
      "Optimal policy is: ['RIGHT', 'RIGHT', 'RIGHT', 'RIGHT', 'UP', 'UP']\n"
     ]
    }
   ],
   "source": [
    "V = [0]*6 # reset the value function\n",
    "\n",
    "for i in range(100): # max iterations\n",
    "    V.append([0]*6)\n",
    "    best_actions = []\n",
    "    for s in range(6): # iterate over all states\n",
    "        action_values = []\n",
    "        for action in action_set:\n",
    "            action_values.append(R[s] + gamma * sum(np.multiply(T[action][s],V[-2])))\n",
    "        V[-1][s] = np.max(action_values)\n",
    "        best_actions.append(action_set[np.argmax(action_values)])\n",
    "    residual = sum(np.subtract(V[-1], V[-2]))\n",
    "    if residual < 0.1:\n",
    "        break\n",
    "print 'Optimal value function is:',V[-1]\n",
    "print 'Optimal policy is:', best_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
